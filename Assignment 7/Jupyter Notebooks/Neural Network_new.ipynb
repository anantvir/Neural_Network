{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self,layer_size_list,train_dataset,test_dataset,batch_size):\n",
    "        self.num_layers = len(layer_size_list)\n",
    "        self.layer_size_list = layer_size_list\n",
    "        self.batch_size = batch_size\n",
    "        self.train_dataset = train_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "        np.random.seed(101)\n",
    "        self.weight_list = [np.random.randn(layer_size_list[i+1],layer_size_list[i]) for i in range(len(layer_size_list)-1)]\n",
    "        #self.single_bias_list = [np.random.randn(x,1) for x in layer_size_list[1:]]\n",
    "        self.bias_list = self.initalize_biases()\n",
    "        #self.last_batch_biases = []\n",
    "        self.no_of_layers = len(layer_size_list)\n",
    "\n",
    "    def initalize_biases(self):\n",
    "        bias_list = [np.empty((x.shape[0],self.batch_size)) for x in self.single_bias_list]\n",
    "        for i,b in enumerate(bias_list):\n",
    "            b[:,:] = self.single_bias_list[i]\n",
    "        return bias_list\n",
    "        \n",
    "    def sigmoid(self,z):\n",
    "        return 1/(1+np.exp(-z))\n",
    "\n",
    "    def feedforward(self,mini_batch):\n",
    "        activations_list = [0] * self.no_of_layers\n",
    "        activations_list[0] = mini_batch.T\n",
    "        prev_layer_activations = activations_list[0]\n",
    "        for layer in range(1,self.no_of_layers):     # l= 2 to L\n",
    "            WX = np.dot(self.weight_list[layer-1],prev_layer_activations)   \n",
    "            bias_matrix = np.empty((WX.shape))\n",
    "            bias_matrix[:,:] = self.bias_list[layer - 1]\n",
    "            Z = WX + self.bias_list[layer-1]\n",
    "            sigmoid_Z = self.sigmoid(Z)\n",
    "            activations_list[layer] = sigmoid_Z\n",
    "            prev_layer_activations = sigmoid_Z\n",
    "        return activations_list\n",
    "\n",
    "    def delta_output_layer(self,mini_batch,mini_batch_labels):\n",
    "        deltas_list = [0] * self.no_of_layers\n",
    "        true_labels_matrix = np.zeros((mini_batch.shape[0],self.layer_size_list[-1]))\n",
    "        for index,row in enumerate(mini_batch):\n",
    "            true_labels_matrix[index][int(mini_batch_labels[index])-1] = 1\n",
    "        activations_list = self.feedforward(mini_batch[:,:400])\n",
    "        output_layer_activations = activations_list[-1]\n",
    "        sigma_prime_output_layer = output_layer_activations * (1 - output_layer_activations)\n",
    "        delta_output_layer = (output_layer_activations - true_labels_matrix.T) * sigma_prime_output_layer\n",
    "        deltas_list[self.no_of_layers - 1] = delta_output_layer\n",
    "        return deltas_list,activations_list\n",
    "             \n",
    "    def backpropagate(self,mini_batch,mini_batch_labels):\n",
    "        deltas_list,activations_list = self.delta_output_layer(mini_batch,mini_batch_labels)\n",
    "        upstream_gradient = deltas_list[-1]\n",
    "        for layer in range(self.no_of_layers,1,-1):\n",
    "            downstream_gradient = np.dot(self.weight_list[layer-2].T,upstream_gradient) * (activations_list[layer-2]*(1 - activations_list[layer-2]))\n",
    "            deltas_list[layer-2] = downstream_gradient\n",
    "            upstream_gradient = downstream_gradient\n",
    "        return deltas_list,activations_list\n",
    "            \n",
    "    def SGD(self,learning_rate):\n",
    "        lr = learning_rate\n",
    "        \"\"\"--------------- Generate Mini Batches -----------------\"\"\"\n",
    "\n",
    "        for batch in range(0,len(self.train_dataset),self.batch_size):\n",
    "            if batch > len(self.train_dataset) - self.batch_size:\n",
    "                mini_batch = self.train_dataset[batch:][:,:400]\n",
    "                mini_batch_labels = self.train_dataset[batch:][:,400]\n",
    "            else:\n",
    "                mini_batch = self.train_dataset[batch:batch + self.batch_size][:,:400]\n",
    "                mini_batch_labels = self.train_dataset[batch:batch + self.batch_size][:,400]\n",
    "            print(mini_batch.shape)\n",
    "            deltas_list,activations_list = self.backpropagate(mini_batch,mini_batch_labels)\n",
    "            \n",
    "            for layer in range(self.no_of_layers,1,-1):\n",
    "                self.weight_list[layer-2] -= lr * np.dot(deltas_list[layer-1],activations_list[layer-2].T)\n",
    "                #print('self.bias_list[layer-2]',self.bias_list[layer-2].shape)\n",
    "                #print('deltas_list[layer-1]',deltas_list[layer-1].shape)\n",
    "                if batch > len(self.train_dataset) - self.batch_size:\n",
    "                    print(self.last_batch_biases[0].shape)\n",
    "                    print(self.last_batch_biases[1].shape)\n",
    "                    self.last_batch_biases[layer-2] -= lr * deltas_list[layer-1]\n",
    "                else:\n",
    "                    self.bias_list[layer-2] -= lr * deltas_list[layer-1]\n",
    "        \n",
    "                    \n",
    "    def evaluate(self,test_dataset):\n",
    "        prev_layer_activations = test_dataset.T\n",
    "        for index,weight in enumerate(self.weight_list):\n",
    "            WX= np.dot(weight,prev_layer_activations)\n",
    "            bias_matrix = np.empty((WX.shape))\n",
    "            bias_matrix[:,:] = self.bias_list[index]\n",
    "            Z = WX + bias_matrix\n",
    "            sigmoid_Z = self.sigmoid(Z)\n",
    "            prev_layer_activations = sigmoid_Z7                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 400)\n",
      "(64, 400)\n",
      "(64, 400)\n",
      "(64, 400)\n",
      "(64, 400)\n",
      "(64, 400)\n",
      "(64, 400)\n",
      "(64, 400)\n",
      "(64, 400)\n",
      "(64, 400)\n",
      "(64, 400)\n",
      "(64, 400)\n",
      "(64, 400)\n",
      "(64, 400)\n",
      "(64, 400)\n",
      "(64, 400)\n",
      "(64, 400)\n",
      "(64, 400)\n",
      "(64, 400)\n",
      "(64, 400)\n",
      "(64, 400)\n",
      "(64, 400)\n",
      "(64, 400)\n",
      "(64, 400)\n",
      "(64, 400)\n",
      "(64, 400)\n",
      "(64, 400)\n",
      "(64, 400)\n",
      "(64, 400)\n",
      "(64, 400)\n",
      "(64, 400)\n",
      "(64, 400)\n",
      "(64, 400)\n",
      "(64, 400)\n",
      "(64, 400)\n",
      "(64, 400)\n",
      "(64, 400)\n",
      "(64, 400)\n",
      "(64, 400)\n",
      "(64, 400)\n",
      "(64, 400)\n",
      "(64, 400)\n",
      "(64, 400)\n",
      "(64, 400)\n",
      "(64, 400)\n",
      "(64, 400)\n",
      "(64, 400)\n",
      "(64, 400)\n",
      "(64, 400)\n",
      "(64, 400)\n",
      "(64, 400)\n",
      "(64, 400)\n",
      "(64, 400)\n",
      "(64, 400)\n",
      "(64, 400)\n",
      "(64, 400)\n",
      "(64, 400)\n",
      "(64, 400)\n",
      "(64, 400)\n",
      "(64, 400)\n",
      "(64, 400)\n",
      "(64, 400)\n",
      "(32, 400)\n",
      "(25, 32)\n",
      "(10, 32)\n",
      "(25, 32)\n",
      "(10, 32)\n"
     ]
    }
   ],
   "source": [
    "dataset_mat = loadmat(r'D:\\\\Courses\\\\Fall 19\\\\ELEG 815 Statistical Learning\\\\HW7\\DatasetDigit.mat')\n",
    "data = np.array(dataset_mat['X'])\n",
    "labels = np.array(dataset_mat['y'])\n",
    "dataset = np.concatenate((data,labels),axis=1)\n",
    "dataset = shuffle(dataset)\n",
    "train_dataset, test_dataset = train_test_split(dataset, test_size=0.20, random_state=42)\n",
    "NN = NeuralNetwork([400,25,10],train_dataset, test_dataset,64)\n",
    "NN.SGD(learning_rate= 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
