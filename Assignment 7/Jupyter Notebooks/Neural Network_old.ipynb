{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self,layer_size_list,dataset):\n",
    "        self.num_layers = len(layer_size_list)\n",
    "        self.layer_size_list = layer_size_list\n",
    "        np.random.seed(101)\n",
    "        self.weight_list = [np.random.randn(layer_size_list[i+1],layer_size_list[i]) for i in range(len(layer_size_list)-1)]\n",
    "        self.activations_list = [0]*(len(layer_size_list))\n",
    "        self.deltas_list = [0]*(len(layer_size_list))\n",
    "        self.dataset = dataset\n",
    "        self.no_of_layers = len(layer_size_list)\n",
    "\n",
    "    def sigmoid(self,z):\n",
    "        return 1/(1+np.exp(-z))\n",
    "\n",
    "    def feedforward(self,mini_batch):\n",
    "        prev_layer_activations = mini_batch.T\n",
    "        self.activations_list[0] = mini_batch.T\n",
    "        for layer in range(2,len(self.layer_size_list)+1):\n",
    "            WX = np.dot(self.weight_list[layer-2],prev_layer_activations)\n",
    "            bias_shape = WX.shape\n",
    "            Z = WX + np.ones((bias_shape))\n",
    "            sigmoid_Z = self.sigmoid(Z)\n",
    "            self.activations_list[layer-1] = sigmoid_Z\n",
    "            prev_layer_activations = sigmoid_Z\n",
    "\n",
    "    def delta_output_layer(self,mini_batch):\n",
    "        true_labels_matrix = np.zeros((mini_batch.shape[0],self.layer_size_list[-1]))\n",
    "        for index,row in enumerate(mini_batch):\n",
    "            true_labels_matrix[index][int(row[-1])-1] = 1\n",
    "        self.feedforward(mini_batch[:,:400])\n",
    "        output_layer_activations = self.activations_list[-1]\n",
    "        sigma_prime_output_layer = output_layer_activations * (1 - output_layer_activations)\n",
    "        delta_output_layer = (output_layer_activations - true_labels_matrix.T) * sigma_prime_output_layer\n",
    "        self.deltas_list[self.no_of_layers - 1] = delta_output_layer\n",
    "        return delta_output_layer\n",
    "             \n",
    "    def backpropagation(self,mini_batch):\n",
    "        upstream_gradient = self.delta_output_layer(mini_batch)\n",
    "        for layer in range(len(self.layer_size_list),1,-1):\n",
    "            downstream_gradient = np.dot(self.weight_list[layer-2].T,upstream_gradient) * self.activations_list[layer-2]\n",
    "            self.deltas_list[layer-2] = downstream_gradient\n",
    "            upstream_gradient = downstream_gradient\n",
    "    \n",
    "    def gradient_descent(self,learning_rate):\n",
    "        \"\"\"------------ Update Weights for Mini Batch -----------\"\"\"\n",
    "        lr = learning_rate\n",
    "        for l in range(self.no_of_layers,1,-1):\n",
    "            self.weight_list[l-2] = self.weight_list[l-2] - lr*(np.dot(self.deltas_list[l-1],self.activations_list[l-2].T))\n",
    "        \n",
    "    def train(self,batch_size,epochs):\n",
    "        \"\"\"--------------- Generate Mini Batches -----------------\"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            last = 0\n",
    "            for i in range(len(self.dataset)):\n",
    "                if i % batch_size == 0 and i != 0:\n",
    "                    if i < len(self.dataset) - batch_size:\n",
    "                        mini_batch = self.dataset[last:i+1]\n",
    "                        last = i\n",
    "                    else:\n",
    "                        mini_batch = self.dataset[last:]\n",
    "                        last = i\n",
    "                    backprop = self.backpropagation(mini_batch)\n",
    "                    self.gradient_descent(learning_rate = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "128\n",
      "192\n",
      "256\n",
      "320\n",
      "384\n",
      "448\n",
      "512\n",
      "576\n",
      "640\n",
      "704\n",
      "768\n",
      "832\n",
      "896\n",
      "960\n",
      "1024\n",
      "1088\n",
      "1152\n",
      "1216\n",
      "1280\n",
      "1344\n",
      "1408\n",
      "1472\n",
      "1536\n",
      "1600\n",
      "1664\n",
      "1728\n",
      "1792\n",
      "1856\n",
      "1920\n",
      "1984\n",
      "2048\n",
      "2112\n",
      "2176\n",
      "2240\n",
      "2304\n",
      "2368\n",
      "2432\n",
      "2496\n",
      "2560\n",
      "2624\n",
      "2688\n",
      "2752\n",
      "2816\n",
      "2880\n",
      "2944\n",
      "3008\n",
      "3072\n",
      "3136\n",
      "3200\n",
      "3264\n",
      "3328\n",
      "3392\n",
      "3456\n",
      "3520\n",
      "3584\n",
      "3648\n",
      "3712\n",
      "3776\n",
      "3840\n",
      "3904\n",
      "3968\n",
      "4032\n",
      "4096\n",
      "4160\n",
      "4224\n",
      "4288\n",
      "4352\n",
      "4416\n",
      "4480\n",
      "4544\n",
      "4608\n",
      "4672\n",
      "4736\n",
      "4800\n",
      "4864\n",
      "4928\n",
      "4992\n",
      "64\n",
      "128\n",
      "192\n",
      "256\n",
      "320\n",
      "384\n",
      "448\n",
      "512\n",
      "576\n",
      "640\n",
      "704\n",
      "768\n",
      "832\n",
      "896\n",
      "960\n",
      "1024\n",
      "1088\n",
      "1152\n",
      "1216\n",
      "1280\n",
      "1344\n",
      "1408\n",
      "1472\n",
      "1536\n",
      "1600\n",
      "1664\n",
      "1728\n",
      "1792\n",
      "1856\n",
      "1920\n",
      "1984\n",
      "2048\n",
      "2112\n",
      "2176\n",
      "2240\n",
      "2304\n",
      "2368\n",
      "2432\n",
      "2496\n",
      "2560\n",
      "2624\n",
      "2688\n",
      "2752\n",
      "2816\n",
      "2880\n",
      "2944\n",
      "3008\n",
      "3072\n",
      "3136\n",
      "3200\n",
      "3264\n",
      "3328\n",
      "3392\n",
      "3456\n",
      "3520\n",
      "3584\n",
      "3648\n",
      "3712\n",
      "3776\n",
      "3840\n",
      "3904\n",
      "3968\n",
      "4032\n",
      "4096\n",
      "4160\n",
      "4224\n",
      "4288\n",
      "4352\n",
      "4416\n",
      "4480\n",
      "4544\n",
      "4608\n",
      "4672\n",
      "4736\n",
      "4800\n",
      "4864\n",
      "4928\n",
      "4992\n"
     ]
    }
   ],
   "source": [
    "dataset_mat = loadmat(r'D:\\\\Courses\\\\Fall 19\\\\ELEG 815 Statistical Learning\\\\HW7\\DatasetDigit.mat')\n",
    "data = np.array(dataset_mat['X'])\n",
    "labels = np.array(dataset_mat['y'])\n",
    "dataset = np.concatenate((data,labels),axis=1)\n",
    "NN = NeuralNetwork([400,25,10],dataset)\n",
    "NN.train(64,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
